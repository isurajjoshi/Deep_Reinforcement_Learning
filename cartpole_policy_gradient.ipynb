{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "tBaO_DkE7Kst",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Deep Reinforcement Learning:\n",
        "## Using Monte Carlo Policy Gradient:\n",
        "## Playing Atari Game - Cartpole:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7XyKXo4T7YJd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## importing the libraries:\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uo90GhFf8F8n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f52bca21-1f4b-4da4-b84d-5cce3804cc99"
      },
      "cell_type": "code",
      "source": [
        "## Create our Environment:\n",
        "env = gym.make('CartPole-v1')\n",
        "env = env.unwrapped\n",
        "\n",
        "# Policy Gradient has high Variance , seed for reproducibility:\n",
        "env.seed(1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "pBzwd1-i8pLK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Setup Hyper-Parameters:\n",
        "\n",
        "# Environment Hyper-Parameter:\n",
        "state_size = 4\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Training Hyper-Parameter:\n",
        "max_episodes = 300\n",
        "learning_rate = 0.01\n",
        "gamma = 0.95      # Discount rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RbeaY5BfHaEV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1aff306b-ddb6-488d-c373-dc8654236267"
      },
      "cell_type": "code",
      "source": [
        "print(action_size)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "58U0sPYkHMIo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Definr Pre-Processing Function:\n",
        "\n",
        "# This function takes the reward and perform Discounting:\n",
        "\n",
        "def discount_normalize_rewards(episode_rewards):\n",
        "  discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
        "  cumulative = 0.0\n",
        "  for i in reserved(range(len(episode_rewards))):\n",
        "    cumulative = cumulative * gamma + episode_rewards[i]\n",
        "    discounted_episode_rewards[i] = cumulative\n",
        "    \n",
        "  mean = np.mean(discounted_episode_rewards)\n",
        "  std = np.std(discounted_episode_rewards)\n",
        "  discounted_episode_rewards = (discounted_episode_rewards - mean ) / std\n",
        "  \n",
        "  return discounted_episode_rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FI-GjoRtKAI9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Create our policy gradint Neural Network Model:\n",
        "\n",
        "# Our state which is an array of 4 values will be used as an input.\n",
        "# Our NN is 3 fully connected layers.\n",
        "# Our output activation function is softmax that squashes the outputs to a probability distribution.\n",
        "\n",
        "with tf.name_scope(\"inputs\"):\n",
        "  inputs_ = tf.placeholder(tf.float32,[None,state_size],name='inputs_')\n",
        "  actions = tf.placeholder(tf.float32,[None,action_size],name='actions')\n",
        "  discounted_episode_rewards = tf.placeholder(tf.float32,[None,],name='discounted_episode_rewards')\n",
        "  \n",
        "  # Add this placeholder for having this variable in Tensorboard:\n",
        "  mean_reward = tf.placeholder(tf.float32,name='mean_reward')\n",
        "  \n",
        "  with tf.name_scope('fc1'):\n",
        "    fc1 = tf.contrib.layers.fully_connected(inputs = inputs_,\n",
        "                                          num_outputs = 10,\n",
        "                                          activation_fn = tf.nn.relu,\n",
        "                                          weights_initializer = tf.contrib.layers.xavier_initializer())\n",
        "    \n",
        "  with tf.name_scope('fc2'):\n",
        "    fc2 = tf.contrib.layers.fully_connected(inputs = fc1,\n",
        "                                           num_outputs = action_size,\n",
        "                                           activation_fn = tf.nn.relu,\n",
        "                                           weights_initializer = tf.contrib.layers.xavier_initializer())\n",
        "    \n",
        "  with tf.name_scope('fc3'):\n",
        "    fc3 = tf.contrib.layers.fully_connected(inputs = fc2,\n",
        "                                           num_outputs = action_size,\n",
        "                                           activation_fn = None,\n",
        "                                           weights_initializer = tf.contrib.layers.xavier_initializer())\n",
        "    \n",
        "    \n",
        "  with tf.name_scope('softmax'):\n",
        "    action_distribution = tf.nn.softmax(fc3)\n",
        "    \n",
        "  with tf.name_scope('loss'):\n",
        "    neg_loss_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = fc3 , labels = actions)\n",
        "    loss = tf.reduce_mean(neg_loss_prob * discounted_episode_rewards)\n",
        "    \n",
        "  with tf.name_scope('train'):\n",
        "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aWJn-nwPUJBb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28d31162-c0b8-405d-94ca-4e55fa52dcf5"
      },
      "cell_type": "code",
      "source": [
        "## Setup tensorboard:\n",
        "\n",
        "# setup tensorboard writer:\n",
        "writer = tf.summary.FileWriter(\"/tensorboard/pg/1\")\n",
        "\n",
        "# loses:\n",
        "tf.summary.scalar(\"Loss\",loss)\n",
        "\n",
        "# Reward Mean:\n",
        "tf.summary.scalar(\"Reward Mean\",mean_reward)\n",
        "\n",
        "writer.op = tf.summary.merge_all()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Summary name Reward Mean is illegal; using Reward_Mean instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OoqOpU1oVpi1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Train our Agent:\n",
        "\n",
        "allRewards = []\n",
        "total_rewards = 0\n",
        "maximum_Reward_Recorded = 0\n",
        "episode = 0\n",
        "episode_states , episode_actions , episode_rewards = [],[],[]\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  for episode in range(max_episodes):\n",
        "    \n",
        "    episode_rewards_sum = 0\n",
        "    \n",
        "    # launch the game:\n",
        "    state = env.reset()\n",
        "    \n",
        "    env.render()\n",
        "    \n",
        "    while True:\n",
        "      # choose action a\n",
        "      # we are not in Deterministic environment\n",
        "      # we have output probabilities\n",
        "      \n",
        "      action_probabilities_distribution = sess.run(action_distribution , feed_dict = {input_:state.resize([1,4])})\n",
        "      \n",
        "      # select action w.r.t action probability:\n",
        "      action = np.random.choice(range(action_probability_distribution.shape[1]),p=action_probability_distribution.ravel())\n",
        "      \n",
        "      # perform action:\n",
        "      new_state , reward , done , info = env.step(action)\n",
        "      \n",
        "      # store state , action , reward :\n",
        "      episode_states.append(state)\n",
        "      \n",
        "      # For actions because we output only one (the index) , we need 2 (1 for the action taken):\n",
        "      # we need [0. , 1.] if we take right , not just the index:\n",
        "      action_ = np.zeros(action_size)\n",
        "      action_[action] = 1\n",
        "      \n",
        "      episode_actions.append(action_)\n",
        "      episode_rewards.append(reward)\n",
        "      \n",
        "      if done:\n",
        "        # calculate the sum reward:\n",
        "        episode_rewards_sum = np.sum(episode_rewards)\n",
        "        \n",
        "        allRewards.append(episode_rewards_sum)\n",
        "        \n",
        "        total_rewards.append(allRewards)\n",
        "        \n",
        "        # Mean Reward:\n",
        "        mean_reward = np.divide(total_rewards,episode+1)\n",
        "        \n",
        "        maximumRewardRecorded = np.amax(allRewards)\n",
        "        \n",
        "        print(\"=============================\")\n",
        "        print(\"Episode: \",episode)\n",
        "        print(\"Episode Rewards: \",episode_rewards_sum)\n",
        "        print(\"Mean Reward: \",mean_reward)\n",
        "        print(\"Max Reward: \",maximumRewardRecorded)\n",
        "        \n",
        "        # calculate discounted reward:\n",
        "        discounted_episode_rewards  = discount_normalize_rewards(episode_rewards)\n",
        "        \n",
        "        # feed-forward gradient and back-propogation:\n",
        "        loss_,_ = sess.run([loss,train_opt],feed_dict = {input_:np.vstack(np.array(episodes_states)),\n",
        "                                                        action:np.vstack(np.array(episode_actions)),\n",
        "                                                        discounted_episode_rewards_:discounted_episode_rewards})\n",
        "        \n",
        "        \n",
        "        # write TF summaries:\n",
        "        summary = sess.run(write_op,feed_dict = {input_ : np.vstack(np.array(episode_states)),\n",
        "                                                action : np.vstack(np.array(episode_actions)),\n",
        "                                                discounted_episode_rewards_ : discounted_episode_rewards,\n",
        "                                                mean_reward_ : mean_reward})\n",
        "        \n",
        "        writer.add_summary(summary,episode)\n",
        "        writer.flush()\n",
        "        \n",
        "        \n",
        "        # reset transition stores:\n",
        "        episode_states , episode_actions , episode_rewards = [],[],[]\n",
        "        \n",
        "      state = new_state\n",
        "    \n",
        "    # save model:\n",
        "    if episode % 100 == 0:\n",
        "      saver.save(sess, \"C:/timru/model.ckpt\")\n",
        "      print(\"Model saved\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hNZ-5jhttSnw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  env.reset()\n",
        "  rewards = []\n",
        "  \n",
        "  # Load the Model:\n",
        "  saver.restore(sess,\"C:/timru/model.ckpt\")\n",
        "  \n",
        "  for episode in range(10):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    print(\"***************************\")\n",
        "    print(\"Episode : \",episode)\n",
        "    \n",
        "    while True:\n",
        "      # choose an action a:\n",
        "      action_probability_distribution = sess.run(action_distribution,feed_dict={input_:state.reshape([1,4])})\n",
        "      \n",
        "      print(action_probability_distribution)\n",
        "      \n",
        "      action = np.random.choice(range(action_probability_distribution.shape[1]),p=action_probability_distribution.ravel())\n",
        "      \n",
        "      new_state ,  reward , done , info = env.step(action)\n",
        "      \n",
        "      total_rewards += reward\n",
        "      \n",
        "      if done:\n",
        "        rewards.append(total_rewards)\n",
        "        print(\"Score\",total_rewards)\n",
        "        \n",
        "        break\n",
        "        \n",
        "      state = new_state\n",
        "      \n",
        "  env.close()\n",
        "  \n",
        "  print(\"Score over time :\", str(sum(rewards)/10))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}