{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "cqJODVW_rM8o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reinforcement Learning\n",
        "# Deep Q Learning for Atari Games"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SrP5jNKntk-B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "ba2e425b-d800-49d6-e7ab-033b65a07274"
      },
      "cell_type": "code",
      "source": [
        "# installing retro:\n",
        "!apt-get install pkg-config lua5.1 build-essential libav-tools git\n",
        "\n",
        "!pip install tqdm retrowrapper gym-retro\n",
        "!pip install -U git+git://github.com/frenchie4111/dumbrain.git"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 8%\r\rReading package lists... 8%\r\rReading package lists... 8%\r\rReading package lists... 8%\r\rReading package lists... 70%\r\rReading package lists... 75%\r\rReading package lists... 75%\r\rReading package lists... 76%\r\rReading package lists... 76%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 90%\r\rReading package lists... 90%\r\rReading package lists... 90%\r\rReading package lists... 90%\r\rReading package lists... 90%\r\rReading package lists... 90%\r\rReading package lists... 90%\r\rReading package lists... 90%\r\rReading package lists... 92%\r\rReading package lists... 92%\r\rReading package lists... 92%\r\rReading package lists... 92%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... Done\r\n",
            "\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\n",
            "\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\n",
            "Package libav-tools is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "However the following packages replace it:\n",
            "  ffmpeg\n",
            "\n",
            "E: Package 'libav-tools' has no installation candidate\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: retrowrapper in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: gym-retro in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-retro) (0.10.11)\n",
            "Requirement already satisfied: pyglet==1.*,>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from gym-retro) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.11.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.*,>=1.3.2->gym-retro) (0.16.0)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (3.0.4)\n",
            "Collecting git+git://github.com/frenchie4111/dumbrain.git\n",
            "  Cloning git://github.com/frenchie4111/dumbrain.git to /tmp/pip-req-build-f4f2g2no\n",
            "Building wheels for collected packages: dumbrain\n",
            "  Building wheel for dumbrain (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-tpei3c52/wheels/50/8e/6f/47c68c95113aa8c02ac02bde75673ace7c3d3636842c75fcb6\n",
            "Successfully built dumbrain\n",
            "Installing collected packages: dumbrain\n",
            "  Found existing installation: dumbrain 0.1\n",
            "    Uninstalling dumbrain-0.1:\n",
            "      Successfully uninstalled dumbrain-0.1\n",
            "Successfully installed dumbrain-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bRaHn2cAsEpc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import libraries:\n",
        "import tensorflow as tf      # Deep Learning Framework\n",
        "import numpy as np           # Handling Matrics\n",
        "import retro                 # Retro Environment\n",
        "\n",
        "from skimage import transform\n",
        "from skimage.color import rgb2grey\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lwKrKhPrvtTz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "cc40db5a-5d55-40a9-b217-de41eb23a50c"
      },
      "cell_type": "code",
      "source": [
        "# Create Game Environment:\n",
        "!python -m dumbrain.rl.retro_contest.install_games http://aiml.mikelyons.org/datasets/sonic/Sonic%20Roms.zip "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(download_url='http://aiml.mikelyons.org/datasets/sonic/Sonic%20Roms.zip', romdir='data/roms/')\n",
            "3694592it [00:00, 6669823.91it/s]                 \n",
            "100% 5767168/5767168 [00:00<00:00, 92112350.10it/s]\n",
            "Importing SonicAndKnuckles3-Genesis\n",
            "Importing SonicTheHedgehog2-Genesis\n",
            "Importing SonicTheHedgehog-Genesis\n",
            "Imported 3 games\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ozscKAimy2Gk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env.close() # for closing the running instance of the game"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oCxP00JZwkoQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f4d47179-8fcb-465b-800b-0175d4f47efa"
      },
      "cell_type": "code",
      "source": [
        "# creating environment:\n",
        "env =retro.make(game='SonicTheHedgehog-Genesis')\n",
        "\n",
        "print('The Size of our Frame is : ',env.observation_space)\n",
        "\n",
        "print('The action_size is : ',env.action_space.n)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Size of our Frame is :  Box(224, 320, 3)\n",
            "The action_size is :  12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pwg6LGgsy_y4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "8ac5b162-d939-466e-864e-54b0c60b48a2"
      },
      "cell_type": "code",
      "source": [
        "# creating action matrics using one-hot encoder:\n",
        "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
        "print(possible_actions)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uT1V2PQx0lD0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the Pre-Processing Function:\n",
        "# Grayscale each of our frames (because color does not add important information ).\n",
        "# We normalize pixel values\n",
        "# Finally we resize the preprocessed frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JPa95y7o1eT2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_frame(frame):\n",
        "  grey = rgb2grey(frame)\n",
        "  normalized = grey/255.0\n",
        "  preprocessed_frame = transform.resize(normalized,[110,84])\n",
        "  \n",
        "  return preprocessed_frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HS19dZyb2M5X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Stacking Frames:\n",
        "\n",
        "# First we preprocess frame\n",
        "# Then we append the frame to the deque that automatically removes the oldest frame\n",
        "# Finally we build the stacked state\n",
        "\n",
        "stack_size = 4     # stacking 4 frames at a time\n",
        "\n",
        "# initialize deque with 0 images , an array for each image:\n",
        "stacked_frames = deque([np.zeros((110,84),dtype=int) for i in range(stack_size)],maxlen=4)\n",
        "\n",
        "def stack_frames(stacked_frames,state,is_new_episode):\n",
        "  \n",
        "  # Preprocess Frame:\n",
        "  frame = preprocess_frame(state)\n",
        "  \n",
        "  if is_new_episode:\n",
        "    # clear stacked frame:\n",
        "    stacked_frames = deque([np.zeros((110,84),dtype=int) for i in range(stack_size)],maxlen=4)\n",
        "    \n",
        "    # appending the first frame 4 times as the episode is new , so no frame was there initially\n",
        "    stacked_frames.append(frame)\n",
        "    stacked_frames.append(frame)\n",
        "    stacked_frames.append(frame)\n",
        "    stacked_frames.append(frame)\n",
        "    \n",
        "    # stack frames\n",
        "    stacked_state  = np.stack(stacked_frames,axis=2)\n",
        "    \n",
        "  else:\n",
        "    stacked_frames.append(frame) # automatically removes the oldest frame from the deque\n",
        "    \n",
        "    stacked_state = np.stack(stacked_frames,axis=2)\n",
        "    \n",
        "  return stacked_state , stacked_frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ainDUGpgDoUG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# setting up our Hyper-Parameters:\n",
        "# First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
        "# Then, you'll add the training hyperparameters when you implement the training algorithm.\n",
        "\n",
        "# Model Hyper-Parameters:\n",
        "state_size = [110, 84, 4]      # Our input is a stack of 4 frames hence 110x84x4 (Width, height, channels) \n",
        "action_size = env.action_space.n # 12 possible actions\n",
        "learning_rate =  0.00025      # Alpha (aka learning rate)\n",
        "\n",
        "# Training Hyper-Parameters:\n",
        "total_episodes = 50            # Total episodes for training\n",
        "max_steps = 50000              # Max possible steps in an episode\n",
        "batch_size = 64                # Batch size\n",
        "\n",
        "# Exploration Parameter for epsilon greedy strategy:\n",
        "explore_start = 1.0            # exploration probability at start\n",
        "explore_stop = 0.01            # minimum exploration probability \n",
        "decay_rate = 0.00001           # exponential decay rate for exploration prob\n",
        "\n",
        "# Q Learning Parameters:\n",
        "gamma = 0.9                    # Discounting rate\n",
        "\n",
        "# Memory Hyper-Parameters:\n",
        "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
        "memory_size = 1000000          # Number of experiences the Memory can keep\n",
        "\n",
        "# Pre-Processing Hyper-Parameters:\n",
        "stack_size = 4                 # Number of frames stacked\n",
        "\n",
        "# Modify this if you just want to see training agent:\n",
        "training = True\n",
        "\n",
        "# Turn this to TRUE , if you want to render the environment\n",
        "episode_render = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jef4aQN-GA1V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating Deep Q Learning Neural Network:\n",
        "\n",
        "# We take a stack of 4 frames as input\n",
        "# It passes through 3 convnets\n",
        "# Then it is flatened\n",
        "# Finally it passes through 2 FC layers\n",
        "# It outputs a Q value for each actions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yi2siszZGXce",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQNetwork:\n",
        "  def __init__(self,state_size,action_size,learning_rate,name='DQNetwork'):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.learning_rate = learning_rate\n",
        "    \n",
        "    with tf.variable_scope(name):\n",
        "      # We create the placeholders\n",
        "      # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
        "      # [None, 110, 84, 4]\n",
        "      \n",
        "      self.inputs_ = tf.placeholder(tf.float32,[None,*state_size],name='inputs_')\n",
        "      self.actions_ = tf.placeholder(tf.float32,[None,action_size],name='actions_')\n",
        "      \n",
        "      # Remember that target_Q is the R(s,a) + ymax Qhat(s', a'):\n",
        "      self.target_Q = tf.placeholder(tf.float32,[None],name='target_Q')\n",
        "      \n",
        "      \"\"\"\n",
        "      First convnet:\n",
        "      CNN\n",
        "      ELU   ( Exponential Linear Unit )\n",
        "      \"\"\"\n",
        "      self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
        "                                         filters = 32,\n",
        "                                         kernel_size = [8,8],\n",
        "                                         strides = [4,4],\n",
        "                                         padding = \"VALID\",\n",
        "                                         kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "                                         name = \"conv1\")\n",
        "            \n",
        "      self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\")\n",
        "      \n",
        "      \"\"\"\n",
        "      Second convnet:\n",
        "      CNN\n",
        "      ELU   ( Exponential Linear Unit )\n",
        "      \"\"\"\n",
        "      self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
        "                                   filters = 64,\n",
        "                                   kernel_size = [4,4],\n",
        "                                   strides = [2,2],\n",
        "                                   padding = \"VALID\",\n",
        "                                   kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "                                   name= \"conv2\")\n",
        "      \n",
        "      self.conv2_out = tf.nn.elu(self.conv2,name=\"conv2_out\")\n",
        "      \n",
        "      \"\"\"\n",
        "      Third convnet:\n",
        "      CNN\n",
        "      ELU   ( Exponential Linear Unit )\n",
        "      \"\"\"\n",
        "      self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
        "                                   filters=64,\n",
        "                                   kernel_size=[3,3],\n",
        "                                   strides=[2,2],\n",
        "                                   padding=\"VALID\",\n",
        "                                   kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "                                   name=\"conv3\")\n",
        "      \n",
        "      self.conv3_out = tf.nn.elu(self.conv3,name=\"conv3_out\")\n",
        "      \n",
        "      # Flattening the outputs from 3 Convolution Layers:\n",
        "      \n",
        "      self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
        "      \n",
        "      # Fully Connected Layer:\n",
        "      \n",
        "      self.fc = tf.layers.dense(inputs=self.flatten,\n",
        "                          units=512,\n",
        "                          activation=tf.nn.elu,\n",
        "                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                          name=\"fc1\")\n",
        "      \n",
        "      self.output = tf.layers.dense(inputs = self.fc, \n",
        "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                                          units = self.action_size, \n",
        "                                        activation=None)\n",
        "      \n",
        "      # Q is our predicted Q value:\n",
        "        \n",
        "      self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_))\n",
        "            \n",
        "      # The loss is the difference between our predicted Q_values and the Q_target\n",
        "      # Sum(Qtarget - Q)^2\n",
        "        \n",
        "      self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
        "            \n",
        "      self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cAkcwWdn2CFF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reset the Graph:\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Initiating the DQNetwork:\n",
        "DQNetwork = DQNetwork(state_size,action_size,learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y5i4ZJBI3KiC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Experience Replay:\n",
        "\n",
        "# Here we'll create the Memory object that creates a deque.\n",
        "# A deque (double ended queue) is a data type that removes the oldest element each time that you add a new element."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yypemvcZ4fyy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Memory():\n",
        "  def __init__(self,max_size):\n",
        "    self.buffer = deque(maxlen=max_size)\n",
        "    \n",
        "  def add(self,experience):\n",
        "    self.buffer.append(experience)\n",
        "    \n",
        "  def sample(self,batch_size):\n",
        "    buffer_size = len(self.buffer)\n",
        "    index = np.random.choice(np.arrange(buffer_size),\n",
        "                            size=batch_size,\n",
        "                            replace=False)\n",
        "    \n",
        "    return [self.buffer[i] for i in index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xZrjI2a36mkH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Here we'll deal with the empty memory problem: \n",
        "# we pre-populate our memory by taking random actions and storing the experience (state, action, reward, next_state)."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "buvk3chK7AgR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# instantiate Memory:\n",
        "memory = Memory(max_size=memory_size)\n",
        "for i in range(pretrain_length):\n",
        "  # if its first step:\n",
        "  \n",
        "  if i==0:\n",
        "    state = env.reset()\n",
        "    \n",
        "    state,stacked_frames = stack_frames(stacked_frames,state,True)\n",
        "    \n",
        "  # get the next state , reward , by taking a random action:\n",
        "  choice = random.randint(1,len(possible_actions))-1\n",
        "  action = possible_actions[choice]\n",
        "  next_state,reward,done,_=env.step(action)\n",
        "  \n",
        "  # env.render()\n",
        "  \n",
        "  # stack the frames:\n",
        "  next_state , stacked_frames = stack_frames(stacked_frames,next_state,False)\n",
        "  \n",
        "  # if the episode is finished:\n",
        "  if done:\n",
        "    # we finished the episode:\n",
        "    next_state = np.zeros(state.shape)\n",
        "    \n",
        "    # adding expeience to memory:\n",
        "    memory.add((state,action,reward,next_state,done))\n",
        "    \n",
        "    # start a new episode:\n",
        "    state = env.reset()\n",
        "    \n",
        "    # stack the frames:\n",
        "    state , stacked_frames = stack_frames(stacked_frames,state,True)\n",
        "    \n",
        "  else:\n",
        "    # Add experience to memory\n",
        "    memory.add((state, action, reward, next_state, done))\n",
        "        \n",
        "    # Our new state is now the next_state\n",
        "    state = next_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "puwDk6-4E6fT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Setup Tensorboard:\n",
        "\n",
        "# Setup TensorBoard Writer\n",
        "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
        "\n",
        "## Losses\n",
        "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
        "\n",
        "write_op = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nExxb0d4FRw8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train our agent:\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This function will do the part\n",
        "With ϵϵ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
        "\"\"\"\n",
        "\n",
        "def predict_action(explore_start,explore_stop,decay_rate,decay_step,state,actions):\n",
        "  ## EPSILON GREEDY STRATEGY\n",
        "  # Choose action a from state s using epsilon greedy.\n",
        "  ## First we randomize a number\n",
        "  \n",
        "  exp_exp_tradeoff = np.random.rand()\n",
        "  \n",
        "  # Using improved version of Exploration Probability:\n",
        "  explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
        "  \n",
        "  if (explore_probability > exp_exp_tradeoff):\n",
        "    # Make a random action ( exploration )\n",
        "    choice = random.randint(1,len(possible_actions))-1\n",
        "    action = possible_actions[choice]\n",
        "    \n",
        "  else:\n",
        "    # Get action from Q-network (exploitation)\n",
        "    # Estimate the Qs values state\n",
        "    Qs = sess.run(DQNetwork.output,feed_dict = {DQNetwork.inputs_ : state.reshape((1,*state.shape))})\n",
        "    \n",
        "    # Take the biggest Q value (= the best action)\n",
        "    choice = np.argmax(Qs)\n",
        "    action = possible_action[choice]\n",
        "    \n",
        "  return action,explore_probability"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rUCmO3_aHgNr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver()      # saving the model\n",
        "\n",
        "if training == True:\n",
        "  with tf.Session() as sess:\n",
        "    # initialize the variables:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # initialize the decay rate to reduce epsilon:\n",
        "    decay_step = 0\n",
        "    \n",
        "    for episode in range(total_episodes):\n",
        "      # set step equals to zero\n",
        "      \n",
        "      step =0\n",
        "      \n",
        "      # initialize the rewards of the episode:\n",
        "      episode_rewards = []\n",
        "      \n",
        "      # Make a new episode and observe the first state:\n",
        "      state = env.reset()\n",
        "      \n",
        "      state , stacked_frames = stack_frames(stacked_frames,state,True)\n",
        "      \n",
        "      while step < max_steps:\n",
        "        step += 1\n",
        "        \n",
        "        # increase decay step:\n",
        "        decay_step += 1\n",
        "        \n",
        "        # predict the action to take , and take it:\n",
        "        action , explore_probability = predict_action(explore_start,explore_stop,decay_rate,decay_step,state,possible_actions)\n",
        "        \n",
        "        # perfor the action and get the next_state,reward and done information:\n",
        "        next_state,reward,done,_ = env.step(action)\n",
        "        \n",
        "        if episode_render:\n",
        "          env.render()\n",
        "          \n",
        "        # add the reward to total reward:\n",
        "        episode.rewards.append(reward)\n",
        "        \n",
        "        # if game is finished:\n",
        "        if done:\n",
        "          # the episode ends , so no next state:\n",
        "          next_state = np.zeroes((110,84),dtype=np.int)\n",
        "          \n",
        "          next_state , stacked_frames = stack_frames(stacked_frames,next_state,False)\n",
        "          \n",
        "          # set step equals max steps to end the episode:\n",
        "          step = max_steps\n",
        "          \n",
        "          # get the total reward of the episode:\n",
        "          total_reward = np.sum(episode_rewards)\n",
        "          \n",
        "          print('Episode: {}'.format(episode),\n",
        "               'Total Reward: {}'.format(total_reward),\n",
        "               'Explore P: {:.4f}'.format(explore_probability),\n",
        "               'Training Loss {:.4f}'.format(loss))\n",
        "          \n",
        "          rewards_list.append((episode,total_rewards))\n",
        "          \n",
        "          # store transition <st,at,rt+1,st+1> in memory D\n",
        "          memory.add((state,action,reward,next_state,done))\n",
        "          \n",
        "          \n",
        "        else:\n",
        "          # stack the frame of next state:\n",
        "          next_state , stacked_frames = stack_frames(stacked_frames,next_state,False)\n",
        "            \n",
        "          # adding experience into memory:\n",
        "          memory.add((state,action,reward,next_state,done))\n",
        "            \n",
        "          # st+1 is now our new state:\n",
        "          state = next_state\n",
        "            \n",
        "        ## Learning Part:\n",
        "        # obtain random mini-batch from the memory:\n",
        "          \n",
        "        batch = memory.sample(batch_size)\n",
        "        states_mb = np.array([each[0] for each in batch],ndim=3)\n",
        "        actions_mb = np.array([each[1] for each in batch])\n",
        "        rewards_mb = np.array([each[2] for each in batch]) \n",
        "        next_states_mb = np.array([each[3] for each in batch],ndim=3)\n",
        "        dones_mb = np.array([each[4] for each in batch])\n",
        "          \n",
        "        target_Qs_batch = []\n",
        "          \n",
        "        # get Q values for next_state:\n",
        "        Qs_next_state = DQNetwork(DQNetwork.output,feed_dict={DQNetwork.inputs_:next_states_mb})\n",
        "          \n",
        "        # set Q_target equals r if episode ends at st+1 , else Q_target = r + gamma * max Q(s',a'):\n",
        "          \n",
        "        for i in range(0,len(batch)):\n",
        "          terminals = dones_mb[i]\n",
        "            \n",
        "          # if we are in termina state , then only eqauls to reward:\n",
        "          if terminal:\n",
        "            target_Qs_batch.append(rewards_mb[i])\n",
        "              \n",
        "          else:\n",
        "            target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
        "            target_Qs_batch.append(target)\n",
        "              \n",
        "          targets_mb = np.array([each for each in target_Qs_batch])\n",
        "            \n",
        "          loss,_ = sess.run([DQNetwork.loss,DQNetwork.optimizer],feed_dict={DQNetwork.inputs_:states_mb,\n",
        "                                                                             DQNetwork.target_Q:targets_mb,\n",
        "                                                                             DQNetworks.actions:actions_mb})\n",
        "            \n",
        "          writer.add_summary(summary,episode)\n",
        "          writer.flush()\n",
        "            \n",
        "      # save model at every 5 steps:\n",
        "      if episode%5 == 0:\n",
        "        save_path = saver.save(sess,\"C:/timru/dqn.ckpt\")\n",
        "        print(\"Model Saved\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HzorhNe-VHLm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test and watch our agent play:\n",
        "\n",
        "with tf.Session as sess:\n",
        "  total_test_rewards = []\n",
        "  \n",
        "  # Load the model:\n",
        "  saver.restore(sess,\"C:/timru/dqn.ckpt\")\n",
        "  \n",
        "  for episode in range(1):\n",
        "    total_rewards = 0\n",
        "    \n",
        "    state = env.reset()\n",
        "    \n",
        "    state ,stacked_frames = stack_frames(stacked_frames , state ,True)\n",
        "    \n",
        "    print(\"Episode \",episode)\n",
        "    \n",
        "    while True:\n",
        "      # reshape the state:\n",
        "      state = state.reshape((1,*state_size))\n",
        "      \n",
        "      # get action from Q-Network:\n",
        "      Qs = sess.run(DQNetwork.output,feed_dict={dQNetwork.inputs_ : state})\n",
        "      \n",
        "      # take the biggest Q value ( the best action )\n",
        "      choice =argmax(Qs)\n",
        "      action = possible_actions[choice]\n",
        "      \n",
        "      # perform the action and get the next state , reward and done information:\n",
        "      next_state , reward , done, _ = env.step(action)\n",
        "      env.render()\n",
        "      \n",
        "      total_rewards += reward\n",
        "      \n",
        "      if done:\n",
        "        print(\"Score = \",total_rewards)\n",
        "        total_test_rewards.append(total_rewards)\n",
        "        break\n",
        "        \n",
        "      next_state , stacked_frames = stack_frames(stacked_frames , next_state ,done , False)\n",
        "      state = next_state\n",
        "      \n",
        "  env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}